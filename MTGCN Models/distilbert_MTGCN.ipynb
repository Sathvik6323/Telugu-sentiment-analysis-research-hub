{"metadata":{"colab":{"name":"distilbert_for_sst.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8301710,"sourceType":"datasetVersion","datasetId":4931986},{"sourceId":8303097,"sourceType":"datasetVersion","datasetId":4932653},{"sourceId":8303318,"sourceType":"datasetVersion","datasetId":4932750},{"sourceId":8304115,"sourceType":"datasetVersion","datasetId":4933084},{"sourceId":8304736,"sourceType":"datasetVersion","datasetId":4933367},{"sourceId":8311280,"sourceType":"datasetVersion","datasetId":4937346}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The Transformer is the latest advance in Deep Learning architectures that has driven most state-of-the-art progress in NLP since it was first presented in ['Attention is All You Need'](https://arxiv.org/abs/1706.03762). Since then, ever larger models are being made, with parameters running into the billions. \n\n> Side-note: I think we're inflection point in ML with OpenAI's release of their API - everyone now has easy access to these state-of-the-art language models, we're gonna see an explosion of use-cases + value creation\n\n\nThere's a lot of greats resources with visualisations to help understand the architecture which I'll come back to. First, a brief introduction to what makes Transformers so powerful:\n\n*   *Self-attention*: a mechanism allowing us to learn contextual relationships between different elements in our input sequence, replacing the need for sequential structure (from RNN/LSTM cells).\n*   *Multi-headed attention*: multiple heads of the model carry out self-attnetion, attending to information jointly at different parts of the sequence from different subspaces. This allows us to learn a variety of features of language + means the model can scale efficiently with large datasets + unsupervised learning.\n* *Transfer learning*: Transformers use the knowledge extracted from a prior setting (usually in the form a language model), which can be unsupervised, then apply or *transfer* to a specific domain, where labelled data is available. This allows a large rich corpus of text to be used in the first pre-training stage, before the model is fine-tuned on custom data. \n\n*insert pre training photo*\n\nIn this post, we'll look at how to fine tune a pre-trained model for the task fo sentiment analysis using Hugging Face's [Transformer](https://huggingface.co/transformers/pretrained_models.html) library, that gives simple access to many of the top transformed-based models (*BERT*, *GPT-2*, *XLNet* etc).  We'll use *DistilBert* here, a lightweight version of the famous *BERT* model with 66 million parameters that's slightly easier to run on a single Colab GPU.\n\nBERT stands for Bidirectional Encoder Representations from Transformers. It uses a *masked* language model where 15% of a sequence's tokens are randomly masked, then the model learns to predict, given a token, what came before *or* after it (the bi-dircectional part). In addition, it has a next sentence prediction objective (did this sentence come after a previous one). BERT differs from a more standard *casual* language model, that predicts the most likely next token in the sequence in a left-to-right direction.\n","metadata":{"id":"bPnTTTnfCWVB","colab_type":"text"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"4EXPFuR-CakA","colab_type":"text"}},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"KlN_8b6FVTPb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"4dff9edc-fa81-4165-bb90-19deff538b39","execution":{"iopub.status.busy":"2024-05-04T11:32:48.021559Z","iopub.execute_input":"2024-05-04T11:32:48.021928Z","iopub.status.idle":"2024-05-04T11:33:01.348123Z","shell.execute_reply.started":"2024-05-04T11:32:48.021896Z","shell.execute_reply":"2024-05-04T11:33:01.346944Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"#from google.colab import drive # import drive from google colab","metadata":{"id":"Pw1lTXEJo5Fd","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:30:25.150613Z","iopub.execute_input":"2024-05-04T11:30:25.150939Z","iopub.status.idle":"2024-05-04T11:30:25.155745Z","shell.execute_reply.started":"2024-05-04T11:30:25.150910Z","shell.execute_reply":"2024-05-04T11:30:25.154780Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#ROOT = \"/content/drive\"     # default location for the drive\n#print(ROOT)                 # print content of ROOT (Optional)\n\n#drive.mount(ROOT)           # we mount the google drive at /content/drive","metadata":{"id":"EDLWrlLoo6TA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"1898f03b-ff12-4647-e393-98f90bd22754","execution":{"iopub.status.busy":"2024-05-04T11:30:25.156996Z","iopub.execute_input":"2024-05-04T11:30:25.157276Z","iopub.status.idle":"2024-05-04T11:30:25.164200Z","shell.execute_reply.started":"2024-05-04T11:30:25.157253Z","shell.execute_reply":"2024-05-04T11:30:25.163279Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:33:01.350362Z","iopub.execute_input":"2024-05-04T11:33:01.350777Z","iopub.status.idle":"2024-05-04T11:33:13.162393Z","shell.execute_reply.started":"2024-05-04T11:33:01.350738Z","shell.execute_reply":"2024-05-04T11:33:13.161214Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nimport torch\nfrom transformers import DistilBertModel, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\n\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom os import path\nimport requests\nimport gzip\nimport zipfile\nimport numpy as np\nfrom collections import defaultdict\n\n# RANDOM_SEED = 0\n# np.random.seed(RANDOM_SEED)\n# torch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"sswx8jnjUh-A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f3584497-77d2-4a9b-fb27-9c2dfb83d747","execution":{"iopub.status.busy":"2024-05-04T11:34:35.511832Z","iopub.execute_input":"2024-05-04T11:34:35.512237Z","iopub.status.idle":"2024-05-04T11:34:35.545160Z","shell.execute_reply.started":"2024-05-04T11:34:35.512206Z","shell.execute_reply":"2024-05-04T11:34:35.544128Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:34:43.114557Z","iopub.execute_input":"2024-05-04T11:34:43.115834Z","iopub.status.idle":"2024-05-04T11:34:48.462845Z","shell.execute_reply.started":"2024-05-04T11:34:43.115793Z","shell.execute_reply":"2024-05-04T11:34:48.461832Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73fce6283bd14f0d9a24a93c0bff609a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffc7cbfa9eeb4f61a66d5a71856dcb17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36d5163dd4b542d1bd4dce4c23f7e8b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0224fbb0a3e14856aa6b189df47c39db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b8e4a0776f469f945783558e2219d0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading our Data\n\nFor the task of sentiment analysis our model takes a sentence as input and outputs one of five classes representing sentiments (very negative, negative, neutral, positive, very positive). The Stanford Sentiment Treebank (SST-5) is the best-known dataset for this, composed of 11855 such sentences with labels 1-5 already split into train, validation and test sets (of sizes 8544, 1101 and 2210). \n\nLet's download the dataset, then split into train/val/test sets.","metadata":{"id":"mN_IQYuGCgdF","colab_type":"text"}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv(\"/kaggle/input/mtgcn-data/modified_file.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:53:41.863571Z","iopub.execute_input":"2024-05-04T11:53:41.864483Z","iopub.status.idle":"2024-05-04T11:53:42.019129Z","shell.execute_reply.started":"2024-05-04T11:53:41.864448Z","shell.execute_reply":"2024-05-04T11:53:42.018189Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Separate features (X) and labels (y)\nX = data[\"review\"].tolist()\ny = data[\"sentiment\"].tolist()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T11:54:13.794082Z","iopub.execute_input":"2024-05-04T11:54:13.794476Z","iopub.status.idle":"2024-05-04T11:54:13.801645Z","shell.execute_reply.started":"2024-05-04T11:54:13.794447Z","shell.execute_reply":"2024-05-04T11:54:13.800388Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\n# Split the data into train, validation, and test sets\nfrom sklearn.model_selection import train_test_split\n\n# Split into train and temporary set (to be further split into validation and test)\nX_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Further split the temporary set into validation and train\nX_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)\n\n# Now you have X_train, y_train, X_val, y_val, X_test, y_test ready for training and evaluation\n","metadata":{"id":"ekH4chUMaG29","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:15.887991Z","iopub.execute_input":"2024-05-04T11:54:15.888365Z","iopub.status.idle":"2024-05-04T11:54:16.443401Z","shell.execute_reply.started":"2024-05-04T11:54:15.888337Z","shell.execute_reply":"2024-05-04T11:54:16.442419Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We need to turn each sequence of words into tokens that serve as inputs into our model. The `DistilBertTokenizer` object does just that. We can see what the tokenizer does to the first sentence in our training set.\n","metadata":{"id":"wJCKt0qEqCmH","colab_type":"text"}},{"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'distilbert/distilbert-base-multilingual-cased'\n\n#tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","metadata":{"id":"BFBvtdiLqDUY","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:17.739644Z","iopub.execute_input":"2024-05-04T11:54:17.740015Z","iopub.status.idle":"2024-05-04T11:54:17.744308Z","shell.execute_reply.started":"2024-05-04T11:54:17.739984Z","shell.execute_reply":"2024-05-04T11:54:17.743366Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sample_txt = str(X_train[0])\ntokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","metadata":{"id":"GMXqosIgizVq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"5904ca63-bf8e-45d1-ae33-96a0b3a64f1b","execution":{"iopub.status.busy":"2024-05-04T11:54:18.156393Z","iopub.execute_input":"2024-05-04T11:54:18.157071Z","iopub.status.idle":"2024-05-04T11:54:18.165769Z","shell.execute_reply.started":"2024-05-04T11:54:18.157035Z","shell.execute_reply":"2024-05-04T11:54:18.164869Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":" Sentence: ఎన్నికల్లో ప్రజలు తెలుగుదేశం పార్టీ కి గట్టి బుద్ది చెప్పబోతున్నారని ఆయన అన్నారు\n   Tokens: ['ఎ', '##న్ని', '##క', '##ల్లో', 'ప్రజలు', 'తెలుగు', '##దేశం', 'ప', '##ార', '##్', '##టీ', 'కి', 'గ', '##ట్టి', 'బ', '##ు', '##ద్', '##ది', 'చ', '##ె', '##ప్', '##ప', '##బ', '##ో', '##తున్న', '##ార', '##ని', 'ఆయన', 'అ', '##న్న', '##ారు']\nToken IDs: [1195, 31114, 15280, 46890, 108356, 45419, 105147, 1220, 67787, 24835, 37551, 11022, 1203, 60660, 1222, 15697, 76859, 19541, 1205, 37637, 50445, 48317, 111337, 21120, 44275, 67787, 13907, 39567, 1188, 37218, 27374]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model needs to account for a few special tokens, namely the start + end of a sentence, unknown words and lastly for padding (each sentence has a different length, not well suited to feed into batches for a deep learning model so we set a suitable max length, then pad shorter sentences up to that length with a padding token.)  All this word is done for us using the `encode_plus` method, which we use to build our `Dataset` object.","metadata":{"id":"z27-S1m5CsQ1","colab_type":"text"}},{"cell_type":"code","source":"class SST_Dataset(Dataset):\n    def __init__(self, ys, Xs, tokenizer, max_len):\n        self.targets = ys\n        self.reviews = Xs\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = str(self.reviews[idx])\n        target = self.targets[idx]\n        encoding = self.tokenizer.encode_plus(\n          review,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          return_tensors='pt',\n          truncation=True\n        )\n        return {\n          'review_text': review,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"id":"jq2tdKD4sFve","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:20.048607Z","iopub.execute_input":"2024-05-04T11:54:20.048984Z","iopub.status.idle":"2024-05-04T11:54:20.056708Z","shell.execute_reply.started":"2024-05-04T11:54:20.048956Z","shell.execute_reply":"2024-05-04T11:54:20.055784Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Next we create our `Dataloader` objects for training, validation and testing. For each item in the dataset we need the encoded input tokens, masks for where the sentence is not padded and the target value.","metadata":{"id":"WLK8xrIJDPnu","colab_type":"text"}},{"cell_type":"code","source":"def create_data_loader(ys, Xs, tokenizer, max_len, batch_size):\n    ds = SST_Dataset(ys, Xs, tokenizer, max_len)\n    return DataLoader(ds, batch_size=batch_size)\n\nBATCH_SIZE = 16\nMAX_LEN = 128\n\ntrain_data_loader = create_data_loader(y_train, X_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(y_val, X_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(y_test, X_test, tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"id":"X9NJsaFAsbov","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:21.427058Z","iopub.execute_input":"2024-05-04T11:54:21.427971Z","iopub.status.idle":"2024-05-04T11:54:21.435568Z","shell.execute_reply.started":"2024-05-04T11:54:21.427933Z","shell.execute_reply":"2024-05-04T11:54:21.434652Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Constructing our model\n\nNow we'ready to build our simple sentiment classification model: we use the output of the `DistilBertModel` - of size 768 - as input into a single fully-connected layer. Dropout is important here for a model with so many parameters (discussed below). (Hugging Face also provide some inbuilt models for downstream tasks that we could have used such as `BertForSequenceClassification` or `BertForQuestionAnswering`)\n","metadata":{"id":"Qh9qCmCXCDDt","colab_type":"text"}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes=3):\n    super(SentimentClassifier, self).__init__()\n    self.bert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n  def forward(self, input_ids, attention_mask):\n    output = self.bert(input_ids, attention_mask)\n    output= output[0][:,0]\n    output = self.drop(output)\n    return self.fc(output)","metadata":{"id":"yO93UREFyktm","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:22.865526Z","iopub.execute_input":"2024-05-04T11:54:22.866201Z","iopub.status.idle":"2024-05-04T11:54:22.872557Z","shell.execute_reply.started":"2024-05-04T11:54:22.866169Z","shell.execute_reply":"2024-05-04T11:54:22.871565Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The BERT authors had some recommendations for hyperparameters when it comes to fine-tuning:\n\n*   *Batch size*: 16, 32\n*   *Learning rate (Adam)*: 5e-5, 3e-5, 2e-5\n*   *Number of epochs*: 2, 3, 4\n\nWe'll largely stick with these - note that the number of epochs is a lot lower than you might expect for a Deep Learning model. This is since we can easily overfit to the training set with many parameters. We'll check for this by calculating both the training and validation accuracy at each epoch. You can find out more about the Hugging Face's optimisers [here](https://huggingface.co/transformers/main_classes/optimizer_schedules.html).","metadata":{"id":"j8DGYeAackFa","colab_type":"text"}},{"cell_type":"code","source":"# initialise model\nmodel = SentimentClassifier()\n\nEPOCHS = 5\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader)*EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=50,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"id":"EM9YYbn1Cx27","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:25.203309Z","iopub.execute_input":"2024-05-04T11:54:25.204008Z","iopub.status.idle":"2024-05-04T11:54:25.715507Z","shell.execute_reply.started":"2024-05-04T11:54:25.203976Z","shell.execute_reply":"2024-05-04T11:54:25.714540Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s continue with writing our helper functions for training our model. ","metadata":{"id":"dkivjBtHElVF","colab_type":"text"}},{"cell_type":"code","source":"def evalModel(model, data_loader, loss_fn, N):\n    \"\"\"Evaluate loss and accuracy of model on data_loader\"\"\"\n    # set model to evaluation mode\n    model = model.eval()\n    total_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            # get inputs and target \n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            # pass through model + make prediction\n            outputs = model(input_ids, attention_mask)\n            _, pred = torch.max(outputs, dim=1)\n\n            # update counters\n            loss = loss_fn(outputs, targets)\n            correct += (pred == targets).sum().item()\n            total_loss += loss.item()*len(targets)\n\n    # normalise\n    return 100*correct/N, total_loss/N","metadata":{"id":"2GKKUTj-I3nf","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:32.074265Z","iopub.execute_input":"2024-05-04T11:54:32.074728Z","iopub.status.idle":"2024-05-04T11:54:32.084186Z","shell.execute_reply.started":"2024-05-04T11:54:32.074686Z","shell.execute_reply":"2024-05-04T11:54:32.083245Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def trainModel(model, trainDataLoader, valDataLoader, loss_fn, optimizer, scheduler, verbose=True):\n    \"\"\"Train sentiment classifier\"\"\"\n    # structure to store progress of the model at each epoch\n    history = defaultdict(list)\n    \n    # move the model to the gpu\n    model = model.to(device)\n\n    for ep in range(EPOCHS):\n        total_loss = 0\n        correct = 0\n        # set model to train mode so dropout and batch normalisation layers work as expected\n        model.train()\n\n        for d in trainDataLoader:\n            # get inputs for batch\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            # calculate output + loss\n            model.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = loss_fn(outputs.squeeze(), targets.long())\n\n            # take gradient step\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n\n            # update losses\n            _, pred = torch.max(outputs, dim=1)\n            correct += (pred == targets).sum().item()\n            total_loss += loss.item()*len(targets)\n\n        #after each epoch, collect statistics\n        history['train_acc'].append(100*correct/len(X_train))\n        history['train_loss'].append(total_loss/len(X_train))\n\n        # statistics about the validation set\n        val_acc, val_loss = evalModel(model, valDataLoader, loss_fn, len(X_val))\n        history['val_acc'].append(val_acc)\n        history['vall_loss'].append(val_loss)\n\n        #if validation improved, save new best model\n        if history['val_acc'][-1] == max(history['val_acc']):\n            print (\"=> Saving a new best at epoch:\", ep)\n            torch.save(model.state_dict(), 'best_model_state.bin')\n        \n        if verbose:\n            print('Epoch {}/{}'.format(ep+1, EPOCHS))\n            print('-' * 10)\n            print('Train loss {} accuracy {}'.format(history['train_loss'][-1], history['train_acc'][-1]))\n            print('Val loss {} accuracy {}'.format(val_loss, val_acc))\n\n    #clean up\n    model = model.to(torch.device(\"cpu\"))\n    del input_ids, attention_mask, targets, outputs, _, pred\n\n    return model, history","metadata":{"id":"7IFnHA0vEJoQ","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T11:54:33.636888Z","iopub.execute_input":"2024-05-04T11:54:33.637518Z","iopub.status.idle":"2024-05-04T11:54:33.650205Z","shell.execute_reply.started":"2024-05-04T11:54:33.637488Z","shell.execute_reply":"2024-05-04T11:54:33.649286Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Let's train our model and see how it does on our test set!","metadata":{"id":"EAC5l3DTg8Qm","colab_type":"text"}},{"cell_type":"code","source":"%%time\nbest_model, histories = trainModel(model, train_data_loader, val_data_loader, loss_fn, optimizer, scheduler, verbose=True)","metadata":{"id":"rrpUbMLrPFGl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"31a027fc-bf85-4de7-c45d-75e85eef277f","execution":{"iopub.status.busy":"2024-05-04T11:54:39.952766Z","iopub.execute_input":"2024-05-04T11:54:39.953162Z","iopub.status.idle":"2024-05-04T12:08:50.233659Z","shell.execute_reply.started":"2024-05-04T11:54:39.953133Z","shell.execute_reply":"2024-05-04T12:08:50.232612Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"=> Saving a new best at epoch: 0\nEpoch 1/5\n----------\nTrain loss 0.9162647313217672 accuracy 57.602921646746346\nVal loss 0.8019359268959326 accuracy 63.721724285104564\n=> Saving a new best at epoch: 1\nEpoch 2/5\n----------\nTrain loss 0.7397323297020426 accuracy 67.02712957693038\nVal loss 0.8055698023757888 accuracy 65.74192630530659\n=> Saving a new best at epoch: 2\nEpoch 3/5\n----------\nTrain loss 0.62924395664732 accuracy 72.96053879719219\nVal loss 0.8558117336502012 accuracy 66.04068857589985\nEpoch 4/5\n----------\nTrain loss 0.5440891673659446 accuracy 77.06791880098653\nVal loss 0.8869479595809255 accuracy 65.88419405320813\n=> Saving a new best at epoch: 4\nEpoch 5/5\n----------\nTrain loss 0.489029304089254 accuracy 79.78561942705369\nVal loss 0.9197145097454813 accuracy 66.04068857589985\nCPU times: user 13min 53s, sys: 15.7 s, total: 14min 9s\nWall time: 14min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_acc, test_loss = evalModel(best_model.to(device), test_data_loader, loss_fn, len(y_test))","metadata":{"id":"4lD9Eshlh3dX","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T12:08:50.235224Z","iopub.execute_input":"2024-05-04T12:08:50.235519Z","iopub.status.idle":"2024-05-04T12:09:05.498980Z","shell.execute_reply.started":"2024-05-04T12:08:50.235492Z","shell.execute_reply":"2024-05-04T12:09:05.498136Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(test_acc, test_loss)","metadata":{"id":"P7AdreD3ODCc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3b888c7e-ffe0-4f9b-a4a6-2b4962dc089c","execution":{"iopub.status.busy":"2024-05-04T12:09:05.500241Z","iopub.execute_input":"2024-05-04T12:09:05.500626Z","iopub.status.idle":"2024-05-04T12:09:05.505693Z","shell.execute_reply.started":"2024-05-04T12:09:05.500573Z","shell.execute_reply":"2024-05-04T12:09:05.504654Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"66.53862569355528 0.914690301290768\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Evaluate the model on the test set\ndef evaluate_model(model, test_data_loader, loss_fn):\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n\n    with torch.no_grad():\n        for batch in test_data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            loss = loss_fn(outputs, targets)\n\n            total_loss += loss.item() * len(targets)\n            _, predictions = torch.max(outputs, dim=1)\n\n            all_predictions.extend(predictions.cpu().tolist())\n            all_targets.extend(targets.cpu().tolist())\n\n    avg_loss = total_loss / len(test_data_loader.dataset)\n    return all_predictions, all_targets, avg_loss\n\n# Evaluate the model\ntest_predictions, test_targets, test_loss = evaluate_model(best_model, test_data_loader, loss_fn)\n\n# Convert to numpy arrays\ntest_predictions = np.array(test_predictions)\ntest_targets = np.array(test_targets)\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(test_targets, test_predictions, average='weighted')\nrecall = recall_score(test_targets, test_predictions, average='weighted')\nf1 = f1_score(test_targets, test_predictions, average='weighted')\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:09:05.508298Z","iopub.execute_input":"2024-05-04T12:09:05.508575Z","iopub.status.idle":"2024-05-04T12:09:20.520749Z","shell.execute_reply.started":"2024-05-04T12:09:05.508551Z","shell.execute_reply":"2024-05-04T12:09:20.519789Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Precision: 0.6603619444995041\nRecall: 0.6653862569355528\nF1 Score: 0.6608919320480887\n","output_type":"stream"}]},{"cell_type":"markdown","source":"There we have it! We've fine-tuned DistilBert for the task of sentiment classification to over 40% test accuracy in only 5 epochs. We can see that the pre-training step of this Tranformer model produces versatile, useful and high-quality features representing different semantics of language.\n\nHowever we note that this doesn't get us close to [state-of-the-art](https://paperswithcode.com/sota/sentiment-analysis-on-sst-5-fine-grained) on this dataset (55%) - the important lesson here is that we haven't tuned any hyperparameters so finding the best optimizer, learning-rate, droupout amount, adding hidden-layers + number of epochs is what will improve our model. We use the validation set to see what hyperparameters get the best accuracy on that - this estimates how our model will generalise to the unseen test set (see your favourite Learning Theory textbooka as to why this works).\n\nRemember that during training we're trying to find the optima a (> 66,000,000 dimension) hypersurface - there's going to many minima so finding the best one requires some searching. Hyperparameter tuning is an important part of solving any problem with Machine Learning, one you just can't avoid.\n\nAs a final bit of fun, let's see what our model predicts on some raw text - we need to tokenise our custom input then pass it through our trained classifier. Though not a 5 we see the model can correctly identify the review as positive!","metadata":{"id":"g6RXwFFAeSjx","colab_type":"text"}},{"cell_type":"code","source":"review_text = \"నాకు చిత్రం బాగా నచ్చింది .\"","metadata":{"id":"_L8yFZfSc3bw","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T12:09:20.522090Z","iopub.execute_input":"2024-05-04T12:09:20.522384Z","iopub.status.idle":"2024-05-04T12:09:20.526699Z","shell.execute_reply.started":"2024-05-04T12:09:20.522357Z","shell.execute_reply":"2024-05-04T12:09:20.525584Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"encoded_review = tokenizer.encode_plus(\n  review_text,\n  max_length=MAX_LEN,\n  add_special_tokens=True,\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',\n  truncation=True\n)","metadata":{"id":"9IlPQbAdc4NZ","colab_type":"code","colab":{},"execution":{"iopub.status.busy":"2024-05-04T12:09:20.527767Z","iopub.execute_input":"2024-05-04T12:09:20.528053Z","iopub.status.idle":"2024-05-04T12:09:20.538447Z","shell.execute_reply.started":"2024-05-04T12:09:20.528026Z","shell.execute_reply":"2024-05-04T12:09:20.537622Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\nprint(f'Review text: {review_text}')\nprint(f'Sentiment  : {int(prediction.cpu().detach().numpy())}')","metadata":{"id":"gUn9i-SAc7UF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b8bb3a77-9c91-4b60-d4f3-3d8be9bf72a3","execution":{"iopub.status.busy":"2024-05-04T12:09:20.539581Z","iopub.execute_input":"2024-05-04T12:09:20.539880Z","iopub.status.idle":"2024-05-04T12:09:20.588334Z","shell.execute_reply.started":"2024-05-04T12:09:20.539856Z","shell.execute_reply":"2024-05-04T12:09:20.587461Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Review text: నాకు చిత్రం బాగా నచ్చింది .\nSentiment  : 2\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2852791618.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  print(f'Sentiment  : {int(prediction.cpu().detach().numpy())}')\n","output_type":"stream"}]}]}